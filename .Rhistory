# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING,c4 STRING,c5 STRING,c6 STRING, timestamp STRING, requisicao STRING, codigo INT, bytes INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
head(df)
?read.text
?sql
teste <- sql("SELECT host FROM df")
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING,c4 STRING,c5 STRING,c6 STRING, timestamp STRING, requisicao STRING, codigo INT, bytes INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
# sql
teste <- sql("SELECT host FROM df")
head(teste)
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING,c4 STRING,c5 STRING,c6 STRING, timestamp STRING, requisicao STRING, codigo INT, bytes INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
# sql
teste <- sql("SELECT host FROM df")
head(teste)
teste <- sql("SELECT host FROM df")
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
teste <- sql("SELECT host FROM df")
head(teste)
teste <- sql("SELECT host, teste=concat(c2,c3) FROM df")
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING,c4 STRING,c5 STRING,c6 STRING, timestamp STRING, requisicao STRING, codigo INT, bytes INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
# sql
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
teste <- sql("SELECT host, teste=concat(c2,c3) FROM df")
head(teste)
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
teste <- sql("SELECT host, teste=concat(c2,c3) FROM df")
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING,c4 STRING,c5 STRING,c6 STRING, timestamp STRING, requisicao STRING, codigo INT, bytes INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
# sql
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
teste <- sql("SELECT host, teste as concat(c2,c3) FROM df")
head(teste)
teste <- sql("SELECT host, teste as concat(c2,c3) FROM df")
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING,c4 STRING,c5 STRING,c6 STRING, timestamp STRING, requisicao STRING, codigo INT, bytes INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
# sql
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
teste <- sql("SELECT host, concat(c2,c3) as teste FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, concat(c5,c6) as timestamp FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, concat(c5,c6) as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, concat(c4,c5) as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'[(.+)]',1) as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5),) as time FROM df")
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5)) as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5),'dd/mm/yyyy') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5),'[dd/MM/yyyy:HH:MM:ss-z]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5),'[dd/MM/yyyy:HH:mm:ss-z]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5),'[dd/MM/yyyy:HH:mm:ss-zzzz]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, concat(c4,c5) as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5),'[d/M/y:H:m:s-z]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'(.+)') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'[(.+)') as time FROM df")
head(teste)
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING,c4 STRING,c5 STRING,c6 STRING, timestamp STRING, requisicao STRING, codigo INT, bytes INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
# sql
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5),'[d/M/y:H:m:s-z]') as time FROM df")
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'[(.+)') as time FROM df")
head(teste)
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING,c4 STRING,c5 STRING,c6 STRING, timestamp STRING, requisicao STRING, codigo INT, bytes INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
# sql
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,c5),'[d/M/y:H:m:s-z]') as time FROM df")
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'[(.+)') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'.(.+).') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'\[(.+)\]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'\\[(.+)\\]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, regexp_extract(concat(c4,c5),'.(.+).') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(regexp_extract(concat(c4,c5),'.(.+).'),'[d/M/y:H:m:s-z]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(regexp_extract(concat(c4,c5),'.(.+).'),'d/M/y:H:m:s-z') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp(regexp_extract(concat(c4,c5),'.(.+).'),'dd/MM/yyyy:HH:mm:ss-zzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:01-0400','dd/MM/yyyy:HH:mm:ss-zzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:01-0400','dd/MM/yyyy:HH:mm:sszzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp(regexp_extract(concat(c4,' ',c5),'.(.+).'),'dd/MM/yyyy:HH:mm:ss-zzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_date(concat(c4,' ',c5),'[d/M/y:H:m:s-z]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, concat(c4,' ',c5) as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:01 -0400','dd/MM/yyyy:HH:mm:ss zzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:01 -0400') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:01 -0400','dd/MM/yyyy:HH:mm:ss zzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, timestamp(to_timestamp('01/Aug/1995:00:00:01 -0400','dd/MM/yyyy:HH:mm:ss zzzz')) as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, timestamp('01/Aug/1995:00:00:01 -0400','dd/MM/yyyy:HH:mm:ss zzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, timestamp('01/Aug/1995:00:00:01 -0400','dd/MM/yyyy:HH:mm:ss zzzz') as time FROM df")
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('2016-12-31', 'yyyy-MM-dd') as time FROM df")
lhead(teste)
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995', 'dd/MM/yyyy') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01-Aug-1995', 'dd-MM-yyyy') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01-08-1995', 'dd-MM-yyyy') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/08/1995', 'dd/MM/yyyy') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995', 'dd/MM/yyyy') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995', 'dd/M/yyyy') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995', 'dd/MMM/yyyy') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp(concat(c4,c5),'[dd/MMM/yyyy:HH:mm:ss-zzzz]') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp(regexp_extract(concat(c4,c5),'.(.+).'),'dd/MMM/yyyy:HH:mm:ss-zzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00', 'dd/MMM/yyyy:HH') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:00', 'dd/MMM/yyyy:HH:mm:ss') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:00-0400', 'dd/MMM/yyyy:HH:mm:sszzzzz') as time FROM df")
head(teste)
teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp(concat(c4,c5),'[dd/MMM/yyyy:HH:mm:sszzzzz]') as time FROM df")
head(teste)
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING, c4 STRING, c5 STRING, c6 STRING, c7 STRING, c8 STRING, c9 INT, c10 INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
head(df)
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING, c4 STRING, c5 STRING, c6 STRING, c7 INT, c8 INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
head(df)
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING, c4 STRING, c5 STRING, c6 STRING, c7 INT, c8 INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
#head(df)
# sql
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
# para realizar o parse da data são necessarios 3 Ms um para cada letra do mes
# teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp(regexp_extract(concat(c4,c5),'.(.+).'),'dd/MMM/yyyy:HH:mm:ss-zzzz') as time FROM df")
# teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:00-0400', 'dd/MMM/yyyy:HH:mm:sszzzzz') as time FROM df")
logs <- sql("SELECT host, concat(c2,c3) as teste,
to_timestamp(concat(c4,c5),'[dd/MMM/yyyy:HH:mm:sszzzzz]') as timestamp,
c6 as request, c7 as codigo, c8 as bytes
FROM df")
head(logs)
# confere o schema
printSchema(logs)
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(logs, "logs")
# Quantidade de hosts unicos
saida <- sql("SELECT count(host) AS hosts")
# Quantidade de hosts unicos
saida <- sql("SELECT count(host) AS hosts FROM logs")
print(saida)
head(saida)
# Quantidade de hosts unicos
saida <- sql("SELECT count(DISTINCT host) AS hosts FROM logs")
head(saida)
# Quantidade de hosts unicos
saida <- sql("SELECT count(DISTINCT host) AS hosts, sum(bytes) AS Bytes FROM logs")
head(saida)
saida <- sql("SELECT count() AS Erros FROM logs WHERE codigo == 404")
head(saida)
saida <- sql("SELECT count() AS Erros FROM logs WHERE codigo = 404")
head(saida)
# confere o schema
printSchema(logs)
saida <- sql("SELECT count() AS Erros FROM logs WHERE codigo == 404")
head(saida)
saida <- sql("SELECT count(host) AS Erros FROM logs WHERE codigo == 404")
head(saida)
print("teste")
saida <- summarize(groupBy(logs, logs$request), count = n(logs$request))
head(saida)
saida <- arrange(summarize(groupBy(logs, logs$request), count = n(logs$request)), desc(saida$count))
head(saida)
# Quantidade de hosts unicos
print("Número de hosts únicos:")
saida <- sql("SELECT count(DISTINCT host) AS hosts, sum(bytes) AS Bytes FROM logs")
head(saida)
saida <- sql("SELECT count(host) AS hosts, sum(bytes) AS Bytes FROM logs")
head(saida)
# Quantidade de hosts unicos
print("Número de hosts únicos e total de bytes retornados:")
saida <- sql("SELECT count(DISTINCT host) AS 'Quantidade de hosts', sum(bytes) AS 'Total de Bytes' FROM logs")
saida <- sql("SELECT count(DISTINCT host) AS 'Quantidade_hosts', sum(bytes) AS 'Total de Bytes' FROM logs")
saida <- sql("SELECT count(DISTINCT host) AS Quantidade_hosts, sum(bytes) AS 'Total de Bytes' FROM logs")
saida <- sql("SELECT count(DISTINCT host) AS Quantidade_hosts, sum(bytes) AS Total_Bytes FROM logs")
head(saida)
saida <- sql("SELECT count(host) AS Erros FROM logs WHERE codigo == 404")
head(saida)
urls <- summarize(groupBy(logs, logs$request), count = n(logs$request))
urls_erros <- sql("SELECT request AS url FROM logs WHERE codigo == 404")
urls_erros <- filter(logs$codigo = 404)
head(urls_erros)
urls_erros <- filter(logs$codigo = 404)
urls_erros <- filter(logs$codigo == 404)
urls_erros <- sql("SELECT count(request) FROM logs WHERE codigo == 404 GROUP BY request")
head(urls_erros)
urls_erros <- sql("SELECT count(request),request FROM logs WHERE codigo == 404 GROUP BY request")
head(urls_erros)
urls_erros <- sql("SELECT count(request) AS quantidade,request FROM logs WHERE codigo == 404 GROUP BY request ORDER BY quantidade")
head(urls_erros)
urls_erros <- sql("SELECT count(request) AS quantidade,request FROM logs WHERE codigo == 404 GROUP BY request ORDER BY quantidade DESC")
head(urls_erros)
urls_erros <- sql("SELECT row_number(), count(request) AS quantidade,request FROM logs WHERE codigo == 404 GROUP BY request ORDER BY quantidade DESC")
head(urls_erros)
?head
head(urls_erros,num=6L)
urls_erros <- sql("SELECT count(request) AS quantidade,request FROM logs WHERE codigo == 404 GROUP BY request ORDER BY quantidade DESC")
head(urls_erros,num=6L)
head(urls_erros,num=5L)
# confere o schema
printSchema(logs)
print("Quantidade de erros 404 por dia:")
urls_erros <- sql("SELECT count(*) as quantidade, data_format(timestamp, 'y-m-d') as dia FROM logs WHERE codigo == 404")
print("Quantidade de erros 404 por dia:")
urls_erros <- sql("SELECT count(*) as quantidade, date_format(timestamp, 'y-m-d') as dia FROM logs WHERE codigo == 404")
print("Quantidade de erros 404 por dia:")
urls_erros <- sql("SELECT count(*) as quantidade, date_format(timestamp, 'y-m-d') as dia FROM logs WHERE codigo == 404")
print("O total de erros 404:")
saida <- sql("SELECT count(*) AS Erros_404 FROM logs WHERE codigo == 404")
head(saida)
print("Os 5 URLs que mais causaram erro 404")
urls_erros <- sql("SELECT count(request) AS quantidade,request FROM logs WHERE codigo == 404 GROUP BY request ORDER BY quantidade DESC")
head(urls_erros,num=5L)
print("Quantidade de erros 404 por dia:")
urls_erros <- sql("SELECT count(*) as quantidade, date_format(timestamp, 'yyyy-MM-dd') as dia FROM logs WHERE codigo == 404")
urls_erros <- sql("SELECT count(*) AS quantidade FROM logs WHERE codigo == 404")
head(urls_erros)
urls_erros <- sql("SELECT count(*) AS quantidade, date_format(timestamp, 'y') as dia FROM logs WHERE codigo == 404")
urls_erros <- sql("SELECT count(*) AS quantidade, date_format(timestamp, 'y') AS dia FROM logs WHERE codigo == 404")
urls_erros <- sql("SELECT count(*) AS quantidade, date_format(timestamp, 'y') FROM logs WHERE codigo == 404")
# confere o schema
printSchema(logs)
urls_erros <- sql("SELECT date_format(timestamp, 'y') FROM logs WHERE codigo == 404")
head(urls_erros)
urls_erros <- sql("SELECT date_format(timestamp, 'y-m-d') FROM logs WHERE codigo == 404")
head(urls_erros)
urls_erros <- sql("SELECT date_format(timestamp, 'y-MM-dd') FROM logs WHERE codigo == 404")
head(urls_erros)
urls_erros <- sql("SELECT date_format(timestamp, 'y-MM-dd') AS Dia, count(host) FROM logs WHERE codigo == 404")
urls_erros <- sql("SELECT date_format(timestamp, 'y-MM-dd') AS Dia FROM logs WHERE codigo == 404")
head(urls_erros)
urls_erros <- sql("SELECT date_format(timestamp, 'y-MM-dd') AS Dia FROM logs WHERE codigo == 404 GROUP BY Dia")
head(urls_erros)
urls_erros <- sql("SELECT date_format(timestamp, 'y-MM-dd') AS Dia, count(*) FROM logs WHERE codigo == 404 GROUP BY Dia")
head(urls_erros)
print("Quantidade de erros 404 por dia:")
urls_erros <- sql("SELECT date_format(timestamp, 'y-MM-dd') AS Dia, count(*) AS Quantidade
FROM logs
WHERE codigo == 404
GROUP BY Dia
ORDER BY quantidade DESC")
head(urls_erros)
head(urls_erros, num = 60L)
head(urls_erros, num = 100L)
#--------------------------------------------------------------------
# Script criado por Gervasio Gesse Junior em 17/Out/2018
# desafio vaga semantix
#--------------------------------------------------------------------
# Verifica se existe a variável de ambiente do SPARK_HOME
# Se não existir carrega o caminho do SPARK_HOME
if (nchar(Sys.getenv("SPARK_HOME")) < 1) {
Sys.setenv(SPARK_HOME = "/opt/spark")
}
library(SparkR)
# caminho dos arquivos
caminho <- "./arqs"
# # inicia o sparkSession
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "com.databricks:spark-csv_2.11:1.0.3")
# sparkR.session(appName = "Desfio_Semantix",
#                sparkPackages = "org.apache.spark:spark-hive_2.11:2.3.0")
sparkR.session(appName = "Desfio_Semantix")
# Define o schema dos arquivos
stringSchema <- "host STRING, c2 STRING, c3 STRING, c4 STRING, c5 STRING, c6 STRING, c7 INT, c8 INT"
# Cria o spark dataframe
df <- read.df(caminho, source = "csv", header = "false", schema = stringSchema,
inferSchema = "false", na.strings = "NA", delimiter = " "
)
#head(df)
# sql
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(df, "df")
# para realizar o parse da data são necessarios 3 Ms um para cada letra do mes
# teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp(regexp_extract(concat(c4,c5),'.(.+).'),'dd/MMM/yyyy:HH:mm:ss-zzzz') as time FROM df")
# teste <- sql("SELECT host, concat(c2,c3) as teste, to_timestamp('01/Aug/1995:00:00:00-0400', 'dd/MMM/yyyy:HH:mm:sszzzzz') as time FROM df")
logs <- sql("SELECT host, concat(c2,c3) as teste,
to_timestamp(concat(c4,c5),'[dd/MMM/yyyy:HH:mm:sszzzzz]') as timestamp,
c6 as request, c7 as codigo, c8 as bytes
FROM df")
# confere o schema
printSchema(logs)
head(logs)
# Registra o SparkDataFrame como uma view temporaria.
createOrReplaceTempView(logs, "logs")
print("Número de hosts únicos e total de bytes retornados:")
saida <- sql("SELECT count(DISTINCT host) AS Quantidade_hosts, sum(bytes) AS Total_Bytes
FROM logs")
head(saida)
print("O total de erros 404:")
saida <- sql("SELECT count(*) AS Erros_404
FROM logs
WHERE codigo == 404")
head(saida)
print("Os 5 URLs que mais causaram erro 404")
urls_erros <- sql("SELECT count(request) AS quantidade,request
FROM logs
WHERE codigo == 404
GROUP BY request ORDER BY quantidade DESC")
head(urls_erros,num=5L)
print("Quantidade de erros 404 por dia:")
urls_erros <- sql("SELECT date_format(timestamp, 'y-MM-dd') AS Dia, count(*) AS Quantidade
FROM logs
WHERE codigo == 404
GROUP BY Dia
ORDER BY quantidade DESC")
head(urls_erros, num = 60L)
